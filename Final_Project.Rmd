---
title: 'Final Project'
author: "Devansh Agarwal"
date: '`r Sys.Date()`'
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: no
    theme: lumen
    df_print: paged
    code_folding: hide
  pdf_document:
    toc: yes
  html_notebook:
    toc: yes
    toc_float: yes
    number_sections: yes
    theme: lumen
---
# Setup

## Packages
```{r,warning=FALSE,message=FALSE}
library(genius)
library(tidyverse)
library(qdap)
library(tidytext)
library(rvest)
library(gridExtra)
```

## Data Access

```{r}
# Data Scraping

page = "https://en.wikipedia.org/wiki/Grammy_Award_for_Record_of_the_Year"

XPATHgen = "/html/body/div[3]/div[3]/div[5]/div[1]/table"

Music <- data.frame()

for (i in 5:8){ # Loop for each decade
  # Scrape website
  Temp <- page %>% 
    read_html() %>%
    html_nodes(xpath = paste(XPATHgen,'[',i,']',sep = '')) %>%
    html_table(fill = TRUE)
  Music <- rbind(Music,Temp[[1]])
}

rm(Temp,i,page,XPATHgen) # Remove unneeded objects from Environment 
```

I created the data frame by first creating a general Xpath which had the first 
part of the Xpath that all paths for each decade had in common.
I then created a loop that would add the part that was different for each xpath
at the end and read that table, then I added that table to the table 
with all the data. 

## Data Wrangling 

```{r,warning=FALSE,message=FALSE}
# Data Cleaning

Lyrics <- 
  Music %>% 
  na.omit() %>% # Remove NAs
  transmute(
    year = as.numeric(gsub(pattern = '(\\[\\d+\\])',replacement = '',x = `Year[I]`)), # Remove foot notes
    track = Record, # Rename
    artist = `Artist(s)`
  ) 

# Add extra variables

Lyrics <-
  Lyrics %>%
  mutate(decade = paste(substr(year,1,3),'0s',sep = '')) %>%
  add_genius(artist, track, type = "lyrics") 

# Convert to words 

Verse_words <- 
  Lyrics %>%
  unnest_tokens(word, lyric)

# Create Data frames for both sets stop words and combine them

words = c("ba","du","yeah","da","ya","ooh","gonna","na","uh","la","hol")

New_Words = data.frame(word = words)

data("stop_words")

Stop_words <-
  stop_words %>%
  select(word)

Stop_words <- rbind(Stop_words,New_Words) 

# Create Dataframe for Verse words without stopped words

Verse_words_Stopped <-
  Verse_words %>%
  anti_join(Stop_words,by = "word")

# Clean environment 

rm(Music,New_Words,stop_words,Stop_words,words,Lyrics)

# Write to CSV files

# write.csv(Verse_words_Stopped,'Verse_words_Stopped.csv')
# write.csv(Verse_words,'Verse_words.csv')
```

First, I did some basic data cleaning by removing NAs from the data frame using na.omit
and removing footnotes from year using gsub and a regular expression to catch 
brackets and the numbers inside of them. I also renamed Record and Artists to 
track and artist. I then created the decade value by taking the first 3 parts of 
the year variable and adding 0s to the end. I then added the lyrics with
add_genius. Then, I used unnest_tokens to make each case represent a word 
instead of a line. Finally, I created a data frame with all the stop word with
rbind and used antijoin to remove the stop words. 


# Data Visualization 

## Graph 1

```{r,warning=FALSE,message=FALSE}
# Read data back in 

#Verse_words <- read.csv("Verse_words.csv", sep = ",", header=T)

# Data wrangling for first graph 

Graph1 <- 
  Verse_words %>%
  group_by(track,artist,decade) %>%
  summarise(count = n())

# Graph boxplot

Graph1 %>%
  ggplot(aes(x = decade,y = count,fill = decade)) +
  geom_boxplot() +
  ggtitle("Boxplots of Words per Grammy Nominated Song by Decade") +
  ylab("Words per Song") + 
  xlab("Decade") +
  theme(legend.position = "none")
```

It appears that the number of words heavily increased at the start of the 21st 
century. This may be due to improvements in musical technology and being able to
create longer songs. There is also more variation from 2000 onward, this could
be a increase in the variation of music


## Graph 2 

```{r}

rm(Graph1)

# Read in data

#Verse_words_Stopped <- read.csv("Verse_words_Stopped.csv", sep = ",", header=T)

# Data wrangling

Topten <-
  Verse_words_Stopped %>%
  group_by(word) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  slice_max(n = 10,order_by = count)

# Graph data

Topten %>%
  ggplot(aes(x = reorder(word,-count), y = count)) +
  geom_col(fill = "#A4DF85",color = "black") +
  ggtitle(
    "Ten Most Popular Words of Grammy Nominated Songs from 1980 - 2019.") +
  ylab("Count") + 
  xlab("Word")
```

It appears that the most common word is the the word love. It appears twice as
much as the next common word of baby. This is probably because many popular songs
are love songs which is reflected in this graph. Also, love is a common word
that would be used in many different songs while the other words might not be as
common across songs. 



## Graph 3



```{r}

rm(Topten)

# Data wrangling

Wordsbydecade <-
  Verse_words_Stopped %>%
  group_by(decade,word) %>%
  summarise(count = n(),.groups = "drop_last") %>%
  arrange(desc(count)) %>%
  slice_max(order_by = count, n = 10)

# Create graphs for each decade

decades <- unique(Wordsbydecade$decade)

colors <- c("pink","Purple","red","blue")

for (index in 1:length(decades)){
  Graphtemp <-
    Wordsbydecade %>%
    filter(decade == decades[index]) %>%
    ggplot() + 
    aes(x = reorder(word,-count),y = count) +
    geom_col(fill = colors[index]) +
    theme(text = element_text(color = "black"),axis.text.x = element_text(angle = 45,hjust = 1.3)) +
    labs(title = decades[index], x = "Word" , y = "Count")
    assign(paste('Graph',decades[index],sep = ''),Graphtemp)
} 


# Combine graphs

grid.arrange(Graph1980s,Graph1990s,Graph2000s,Graph2010s,nrow = 2, top = "Top Ten Words by Decade")
```

From this graph, you can tell that The across all decades, the word love appears
the most. The more interesting statistic is the words after. For example, in
1990s and the 2000s, the word baby became relatively popular. This most likely
in relation to love songs which utilize baby as a term of endearment. 


## Graph 4

```{r}

rm(Graphtemp,index,decades,colors,Graph1980s,Graph1990s,Graph2000s,Graph2010s,Wordsbydecade)

# Data wrangling 

data('sentiments')

Sentiments <-
  sentiments %>%
  mutate(
    sentiment = ifelse(sentiment == 'positive',1,0)
  )

Music_Sentiments <-
  Verse_words_Stopped %>%
  right_join(Sentiments, by = "word") %>%
  select(decade,year,sentiment) %>%
  group_by(year) %>%
  summarise(decade = first(decade), net_sentiment = sum(sentiment)) %>%
  na.omit()

rm(sentiments,Sentiments)

# Graph

Music_Sentiments %>%
  ggplot(aes(x = year,y = net_sentiment,fill = decade)) +
  geom_col() +
  labs(title = "Net Sentiment Score by Year", x = "Year", y =" Net Sentiment")  
```

It appears that that in each decade, a few years are outliers in the amount of
sentiment they have. For example, the sentiment score of 2011 was a mere 3 while
the sentiment score of 2016 was a whopping 103. This could just be due to the 
individual variations of songs. However, it may also show that the type of songs
we enjoy have a large variation in sentiment. 


## Graph 5

```{r}

# Data Wrangling

Mean_Sentiments <-
  Music_Sentiments %>%
  group_by(decade) %>%
  summarise(mean = mean(net_sentiment))

# Graph

Mean_Sentiments %>%
  ggplot() +
  aes(x = decade,y = mean, fill = decade) +
  geom_col() +
  theme(legend.position = "None") +
  labs(title = "Mean Sentiment Score by Decade",x = "Decade", y = "Mean Sentiment Score")

```

Sentiment by decade appears to have decreased in 1990s and 2000s while it was 
higher in 1980s and 2010s. This could be reflective of the feelings at the time
and how they impact song sentiment per decade.

## Graph 6 
```{r,warning=FALSE,message=FALSE}
# Graph

Music_Sentiments %>%
  ggplot() +
  aes(x = year,y = net_sentiment, color = decade) +
  geom_smooth(se = FALSE, color = "blue") + 
  geom_point() +
  theme_bw() +
  theme(title = element_text(size = 8)) +
  labs(title = "Net Sentiment Score by Year of Grammy Nominated Records from 1980 - 2019", x = "Year",y = "Net Sentiment")
```

This graph shows a simple regression line to show the changes in net sentiment
score by year. It is easy to see that the sentiment score appeared be decreasing
in the 1990s but picked again in mid 2000s. This could be due to natural changes
or due to an important event the occurred during that time period. 

# Webpage
https://supernova128.github.io/Stat184FinalProject/Final_Project.html