---
title: 'Final Project'
author: "Devansh Agarwal"
date: 'Due Date: 12/14/2020'
output:
  html_notebook:
    df_print: paged
  html_document:
    df_print: paged
---
### Setup 
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r}
# Libraries

library(genius)
library(tidyverse)
library(qdap)
library(tidytext)
library(rvest)
library(gridExtra)
```
### Data Access
```{r}
# Data Scraping

page = "https://en.wikipedia.org/wiki/Grammy_Award_for_Record_of_the_Year"

XPATHgen = "/html/body/div[3]/div[3]/div[5]/div[1]/table"

Music <- data.frame()

for (i in 5:8){ # Loop for each decade
  # Scrape website
  Temp <- page %>% 
    read_html() %>%
    html_nodes(xpath = paste(XPATHgen,'[',i,']',sep = '')) %>%
    html_table(fill = TRUE)
  Music <- rbind(Music,Temp[[1]])
}

rm(Temp,i,page,XPATHgen) # Remove unneeded objects from Environment 
```

I created the data frame by first creating a general xpath which had the first 
part of the Xpath that all paths for each decade had in common.
I then created a loop that would add the part that was different for each xpath
at the end, then I added that table to the table with all the data. 

### Data Wrangling 

```{r}
# Data Cleaning

Lyrics <- 
  Music %>% 
  na.omit() %>% # Remove NAs
  transmute(
    year = as.numeric(gsub(pattern = '(\\[\\d+\\])',replacement = '',x = `Year[I]`)), # Remove foot notes
    track = Record,
    artist = `Artist(s)`
  ) %>%
  mutate(
    decade = paste(substr(year,1,3),'0s',sep = '')
  ) %>%
  add_genius(artist, track, type = "lyrics") # Got 174 songs 

# Convert to words 

Verse_words <- 
  Lyrics %>%
  unnest_tokens(word, lyric)

# Create Data frames for both sets stop words and combine them

words = c("ba","du","yeah","da","ya","ooh","gonna","na","uh","la","hol")

New_Words = data.frame(word = words)

data("stop_words")

Stop_words <-
  stop_words %>%
  select(word)

Stop_words <- rbind(Stop_words,New_Words) 

# Create Dataframe for Verse words without stopped words

Verse_words_Stopped <-
  Verse_words %>%
  anti_join(Stop_words,by = "word")

# Clean environment 

rm(Music,New_Words,stop_words,Stop_words,words,Lyrics)

# Write to CSV files

write.csv(Verse_words_Stopped,'Verse_words_Stopped.csv')
write.csv(Verse_words,'Verse_words.csv')
```

First, I did some basic data cleaning by removing NAs from the data frame  and removing footnotes from  year using gsub and a regular expression to catch brackets and the numbers inside of them. I also renamed Record and Artists to track and artist. I then created the decade value by taking the first 3 parts of the year variable and adding 0s to the end. I then added the lyrics with add_genius. Then, I used unnest_tokens to make each case represent a word instead of a line. Finally, I created a data frame with all the stop word with rbind and used antijoin to remove the stop words. Finally I wrote the answers to a CSV file to avoid repeating this program.


### Data Visualization 

#### Graph 1

```{r}
# Read data back in 

Verse_words <- read.csv("Verse_words.csv", sep = ",", header=T)

# Data wrangling for first graph 

Graph1 <- 
  Verse_words %>%
  group_by(track,artist,decade) %>%
  summarise(count = n())

# Graph boxplot

Graph1 %>%
  ggplot(aes(x = decade,y = count,fill = decade)) +
  geom_boxplot() +
  ggtitle("Boxplots of Words per Grammy Nominated Song by Decade") +
  ylab("Words per Song") + 
  xlab("Decade") +
  theme(legend.position = "none")
rm(Graph1)
```
It appears that the number of words heavily increased at the start of the 21st century. 
This may be due to improvements in musical technology and being able to create longer songs. There is also more variation from 2000 onwards, this could be a increase in the variation of music


#### Graph 2 

```{r}

# Read in data

Verse_words_Stopped <- read.csv("Verse_words_Stopped.csv", sep = ",", header=T)

# change data to get the top ten words and how many times they are used 

Topten <-
  Verse_words_Stopped %>%
  group_by(word) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  slice_max(n = 10,order_by = count)

Topten %>%
  ggplot(aes(x = reorder(word,-count), y = count)) +
  geom_col(fill = "#A4DF85",color = "black") +
  ggtitle(
    "Ten Most Popular Words of Grammy Nominated Songs from 1980 - 2019.") +
  ylab("Count") + 
  xlab("Word")
rm(Topten)
```
It appears that the most common word is the the word love. It appears twice as
much as the next common word of baby. This is probably because many popular songs
are love songs which is reflected in this graph. Also, love is a common word that would be used in many different songs while the other words might not be as common across songs. 


#### Graph 3



```{r}

# Create base dataset

Wordsbydecade <-
  Verse_words_Stopped %>%
  group_by(decade,word) %>%
  summarise(count = n(),.groups = "drop_last") %>%
  arrange(desc(count)) %>%
  slice_max(order_by = count, n = 10)

# Create graphs for each decade
decades <- unique(Wordsbydecade$decade)

colors <- c("pink","Purple","red","blue")

for (index in 1:length(decades)){
  Graphtemp <-
    Wordsbydecade %>%
    filter(decade == decades[index]) %>%
    ggplot() + 
    aes(x = reorder(word,-count),y = count) +
    geom_col(fill = colors[index],color = "black") +
    ggtitle(decades[index]) +
    ylab("Count") + 
    xlab("Word")
    assign(paste('Graph',decades[index],sep = ''),Graphtemp)
} 


# Combine graphs

grid.arrange(Graph1980s,Graph1990s,Graph2000s,Graph2010s,nrow = 2, top = "Top Ten Words by Decade")

rm(Graphtemp,index,decades,colors,Graph1980s,Graph1990s,Graph2000s,Graph2010s,Wordsbydecade)
```

#### Graph 4

```{r}
# Data wrangling 
data('sentiments')

Sentiments <-
  sentiments %>%
  mutate(
    sentiment = ifelse(sentiment == 'positive',1,0)
  )

Music_Sentiments <-
  Verse_words_Stopped %>%
  right_join(Sentiments, by = "word") %>%
  select(decade,year,sentiment) %>%
  group_by(year) %>%
  summarise(decade = first(decade), net_sentiment = sum(sentiment)) %>%
  na.omit()

rm(sentiments,Sentiments)

Music_Sentiments %>%
  ggplot(aes(x = year,y = net_sentiment,fill = decade)) +
  geom_col() +
  labs(title = "Net Setiment Score by Year", x = "Year", y =" Net Sentiment")  
```



#### Graph 5

```{r}

Mean_Sentiments <-
  Music_Sentiments %>%
  group_by(decade) %>%
  summarise(mean = mean(net_sentiment))

Mean_Sentiments %>%
  ggplot() +
  aes(x = decade,y = mean, fill = decade) +
  geom_col() +
  theme(legend.position = "None") +
  labs(title = "Mean Sentiment Score by Decade",x = "Decade", y = "Mean Sentiment Score")

```


#### Graph 6 
```{r}
Music_Sentiments %>%
  ggplot() +
  aes(x = year,y = net_sentiment, color = decade) +
  geom_smooth(se = FALSE, color = "blue", lm = TRUE) + 
  geom_point() +
  theme_bw() +
  theme(title = element_text(size = 8)) +
  labs(title = "Net Sentiment Score by Year of Grammy Nominated Records from 1980 - 2019 with Linear Model Fit", x = "Year",y = "Net Sentiment")
```

